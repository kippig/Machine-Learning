{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a Naive Bayes model, finds the optimal value of alpha, and reports F1 and log loss scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip data files into the \"csv\" subdirectory \n",
    "# (unless this has already been done this since running the Data Set Up notebook)\n",
    "\n",
    "# **IMPORTANT**  This will overwrite existing files in the \"csv\" folder in your local repo\n",
    "# with the most recent data files from the data.zip file\n",
    "\n",
    "# Unzip 80% training data\n",
    "unzip_training_data = zipfile.ZipFile(\"data_subset.zip\", \"r\")\n",
    "unzip_training_data.extractall()\n",
    "unzip_training_data.close()\n",
    "\n",
    "# Unzip development and training data\n",
    "unzip_test_data = zipfile.ZipFile(\"testing.zip\", \"r\")\n",
    "unzip_test_data.extractall()\n",
    "unzip_test_data.close()\n",
    "\n",
    "# Unzip full set of training data for creating predictions to submit to Kaggle\n",
    "unzip_all_data = zipfile.ZipFile(\"data.zip\", \"r\")\n",
    "unzip_all_data.extractall()\n",
    "unzip_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for testing on development data\n",
    "train_data = np.loadtxt('csv/train_data.csv', delimiter=\",\")\n",
    "train_labels = np.loadtxt('csv/train_labels.csv', dtype=str, delimiter=\",\")\n",
    "dev_data = np.loadtxt('csv/dev_data.csv', delimiter=\",\")\n",
    "dev_labels = np.loadtxt('csv/dev_labels.csv', dtype=str, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load these csv files into numpy arrays for creating predictions to submit to Kaggle\n",
    "train_data_all = np.loadtxt('csv/train_data_all.csv', delimiter=\",\")\n",
    "train_labels_all = np.loadtxt('csv/train_labels_all.csv', dtype=str, delimiter=\",\")\n",
    "test_data_all = np.loadtxt('csv/test_data_all.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is (702439, 58)\n",
      "train_labels shape is (702439,)\n",
      "dev_data shape is (175610, 58)\n",
      "dev_labels shape is (175610,)\n"
     ]
    }
   ],
   "source": [
    "# print shapes to compare before and after csv conversion\n",
    "print(\"train_data shape is\", train_data.shape)\n",
    "print(\"train_labels shape is\", train_labels.shape)\n",
    "print(\"dev_data shape is\", dev_data.shape)\n",
    "print(\"dev_labels shape is\", dev_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"train_data_all shape is\", train_data_all.shape)\n",
    "print(\"train_labels_all shape is\", train_labels_all.shape)\n",
    "print(\"test_data_all shape is\", test_data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IF there are additional changes to make to the data for this model\n",
    "# that would be easier to do in pandas, uncomment and run this code. \n",
    "# This model works the same whether the data is in numpy or pandas, so presumably so do other models\n",
    "\n",
    "#train_data = pd.DataFrame(train_data)\n",
    "#train_labels = pd.DataFrame(train_labels)\n",
    "#dev_data = pd.DataFrame(dev_data)\n",
    "#dev_labels = pd.DataFrame(dev_labels)\n",
    "#train_data_all = pd.DataFrame(train_data_all)\n",
    "#train_labels_all = pd.DataFrame(train_labels_all)\n",
    "#test_data_all = pd.DataFrame(test_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up functions for training Naive Bayes model and finding optimal value of alpha (smoothing)\n",
    "\n",
    "def TrainNB(data, labels, test_data, alpha_value=1.0):\n",
    "    \"\"\"This function takes in training data and labels, testing data,\n",
    "    and can accept different values of alpha (the amount of smoothing)\n",
    "    It trains a Naive Bayes model and returns the model and predicted probabilities.\n",
    "    \"\"\"\n",
    "    NB = MultinomialNB(alpha=alpha_value)\n",
    "    NB.fit(data, labels)\n",
    "    pp = NB.predict_proba(test_data)\n",
    "    return NB, pp\n",
    "\n",
    "def find_alpha(data, labels, dev_data, dev_labels, alpha_values):\n",
    "    \"\"\"Find optimal value of C in a logistic regression model.  \n",
    "    \n",
    "    Note that this cannot be used on test data from Kaggle \n",
    "    because we do not have labels for that data.  This function is intended to only be used\n",
    "    in the development stage with the development data.\n",
    "    \"\"\"\n",
    "    for a in alpha_values:      \n",
    "        NB, pp = TrainNB(data, labels, dev_data, a)\n",
    "        predictions = NB.predict(dev_data)\n",
    "        f1 = metrics.f1_score(dev_labels, predictions, average = \"weighted\")\n",
    "        logloss = metrics.log_loss(dev_labels, pp)\n",
    "        \n",
    "        # Print F1 score and log loss for each value of k\n",
    "        print(\"For alpha =\", a, \"the F1 score is\", round(f1, 6), \"and the Log Loss score is\", round(logloss, 6))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal value of alpha using the 80% training data and the development data\n",
    "alpha_values = [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]\n",
    "find_alpha(train_data, train_labels, dev_data, dev_labels, alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X must be non-negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ae6f0d112335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Can also train the model with a single value of alpha with 80% training data and development data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mNB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlogloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6dbde4242eed>\u001b[0m in \u001b[0;36mTrainNB\u001b[0;34m(data, labels, test_data, alpha_value)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0mNB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laura/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    602\u001b[0m         self.feature_count_ = np.zeros((n_effective_classes, n_features),\n\u001b[1;32m    603\u001b[0m                                        dtype=np.float64)\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/laura/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input X must be non-negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X must be non-negative"
     ]
    }
   ],
   "source": [
    "# Can also train the model with a single value of alpha with 80% training data and development data\n",
    "a = 0.5\n",
    "NB, pp = TrainNB(train_data, train_labels, dev_data, a)\n",
    "logloss = metrics.log_loss(dev_labels, pp)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before submitting to Kaggle, run the model on the full set of training data and test data\n",
    "# using the optimal value for the model\n",
    "a = use optimal value here\n",
    "NB, pp = TrainNB(train_data_all, train_labels_all, test_data_all, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up predictions for submission to Kaggle\n",
    "headers = [\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "           \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "           \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "           \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "           \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "           \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]\n",
    "data = pd.DataFrame(data=pp, \n",
    "                    index=[x for x in range(len(test_data_all))], \n",
    "                    columns=headers)\n",
    "data.columns.name =\"Id\"\n",
    "print(data)Create zipped csv file for Kaggle\n",
    "#### Update the filename first in all lines of the following code\n",
    "Add something unique after our names to avoid overwriting other submission files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create zipped csv file for Kaggle\n",
    "#### Update the filename first in all lines of the following code\n",
    "Add something unique after our names to avoid overwriting other submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('Williams_Gascoigne_Vignola_.csv', index_label = \"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip_probs = zipfile.ZipFile(\"Williams_Gascoigne_Vignola_.zip\", \"w\")\n",
    "zip_probs.write(\"Williams_Gascoigne_Vignola_.csv\", compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_probs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from previous datasets and/or model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
